{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1bn13c9AbMWI-BGYvaK1MlWGvKU63IjcL",
      "authorship_tag": "ABX9TyOyERgW7RfMMqGHMR6Qc1m/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvanDePivan/2AMM30-groep-2-component-1/blob/main/2AMM30_Text_Mining_1_chess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# not currently using pandas, this option is for forcing a newer version of pandas which has faster processing\n",
        "# !pip install pandas --force-reinstall"
      ],
      "metadata": {
        "id": "pDXCMpnL2T22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install spacy\n",
        "# !pip install spacy-cleaner\n",
        "!pip install ftfy\n",
        "!pip install spacy-transformers\n",
        "!pip install SPARQLWrapper\n",
        "!pip install srsly\n"
      ],
      "metadata": {
        "id": "RlhkUVLQxeq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "_627IovSyh4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfRKGhr3nfKe"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import pyarrow as pa\n",
        "import json\n",
        "from pathlib import Path\n",
        "import spacy_transformers\n",
        "import spacy\n",
        "import ftfy\n",
        "import random\n",
        "import srsly\n",
        "import re\n",
        "from spacy import displacy\n",
        "from spacy.pipeline.spancat import DEFAULT_SPANCAT_MODEL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pathlist = [p for p in Path('drive/MyDrive/enwiki20230820/raw/').glob('**/*') if p.is_file()]\n",
        "nlp = spacy.load('en_core_web_trf')\n",
        "titles = []\n",
        "with open('drive/MyDrive/enwiki20230820/chess_article_titles_no_dupes.txt') as f:\n",
        "  for l in f:\n",
        "    titles.append(ftfy.fix_text(l.strip()))\n",
        "\n",
        "print(f'{len(titles)} titles')\n",
        "pattern = '|'.join(titles)"
      ],
      "metadata": {
        "id": "uSWvKDNhO7kj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f44cd05-b396-491b-c608-34db5bb99e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6941 titles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read files using srsly (generators)\n",
        "from itertools import chain\n",
        "\n",
        "def get_json_readers():\n",
        "  return chain.from_iterable(srsly.read_jsonl(path) for path in pathlist)\n",
        "\n",
        "def read_random_json():\n",
        "  data = srsly.read_jsonl(random.choice(pathlist))\n",
        "  articles = [article for article in data if len(article[\"text\"]) > 0 and re.fullmatch(pattern, article[\"title\"])]\n",
        "\n",
        "  return articles\n",
        "\n",
        "def read_all_json():\n",
        "  articles = []\n",
        "  count_json_files = 0\n",
        "  count_articles = 0\n",
        "  generators = get_json_readers()\n",
        "\n",
        "  for data in generators:\n",
        "    articles.append([article for article in data if len(article[\"text\"]) > 0 and re.fullmatch(pattern, article[\"title\"])])\n",
        "    count_json_files += 1\n",
        "    count_articles += len(articles[-1])\n",
        "\n",
        "    if count_json_files % 10 == 0:\n",
        "      print(f'found {count_articles} nonempty chess articles so far in {count_json_files}/{len(pathlist)} files')\n",
        "\n",
        "  print(f'found {count_articles} nonempty chess articles in {count_json_files}/{len(pathlist)} files')\n",
        "  return articles\n"
      ],
      "metadata": {
        "id": "78_ZDXVvxB0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nonempty_chess = Path('drive/MyDrive/enwiki20230820/chess_nonempty.json')\n",
        "# df = pd.read_json(\n",
        "#     nonempty_chess_csv_path,\n",
        "#     lines=True,\n",
        "#     engine='pyarrow',\n",
        "#     dtype={\"title\": pd.ArrowDtype(pa.string()), \"text\": pd.ArrowDtype(pa.string())}\n",
        "# )\n",
        "srsly.read_jsonl(nonempty_chess)\n"
      ],
      "metadata": {
        "id": "XMEaPGQEfNaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe7d2028-0624-4753-fe11-c4074fff6701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object read_jsonl at 0x7fdc772f6b20>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nlp = spacy.load('en_core_web_trf')\n",
        "#spancat config\n",
        "config = {\n",
        "    #this refers to the minimum probability to consider a prediction positive\n",
        "    \"threshold\": 0.5,\n",
        "    #this refers to the maximum number of labels to consider positive per span\n",
        "    \"max_positive\": None,\n",
        "     #a model instance that is given a list of documents with start end indices representing the labelled spans\n",
        "    \"model\": DEFAULT_SPANCAT_MODEL,\n",
        "    #A function that suggests spans. This suggester is fixed n-gram length of up to 3 tokens\n",
        "    \"suggester\": {\"@misc\": \"spacy.ngram_suggester.v1\", \"sizes\": [1, 2, 3]},\n",
        "}\n",
        "from spacy.lookups import Lookups\n",
        "\n",
        "#add spancat component to nlp object\n",
        "# nlp.add_pipe(\"spancat\")\n",
        "# nlp.initialize()"
      ],
      "metadata": {
        "id": "6EQ_YwyzQXPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regexeses = [\"(\\d{1,3}\\.)?([KQBNPR]?[abcdefghx]+[12345678]\\+?\\!?\\??#?)|O-O(-O)?|\\d\\-\\d\", # Removes chess notation\n",
        "             '^(\\d+\\. *)+', # Removes numbered list indicatords (e.g. 1. 2. etc)\n",
        "             '^\\+', # Removes the + at the start of some sentences\n",
        "             '\\(.?\\)'] # Removes empty/1 character parenthesis\n",
        "\n",
        "reg = '|'.join(regexeses)\n",
        "empty_parenthesis_regex = '\\(([;,] ?)+' # Removes the artifacts left behind from some previous filtering (e.g. \"(; ; Thomas edison)\" -> \"(Thomas edison)\"\")\n",
        "def clean_text(text):\n",
        "  sentences = ftfy.fix_text(text)\n",
        "  sentences = re.sub(reg, '', sentences)\n",
        "  sentences = re.sub('([12]\\d)(\\d{2}â€“)(\\d{2}\\D)', '\\1\\2\\1\\3', sentences) # expands a sequence containing 4 numbered years followed by 2 numbered years. (e.g. 1976-78 -> 1976-1978)\n",
        "  sentences = re.sub(empty_parenthesis_regex, '(', sentences)\n",
        "  sentences = re.sub('\\n.?\\n', '\\n', sentences) # remove 1 character lines\n",
        "  sentences = re.sub('\\n', ' ', sentences) # Remove newlines\n",
        "  sentences = re.sub(' +', ' ', sentences) # Remove weird lines that only have a +\n",
        "  return sentences\n",
        "\n",
        "def clean_json(obj):\n",
        "  obj[\"text\"] = clean_text(obj[\"text\"])\n",
        "  return obj"
      ],
      "metadata": {
        "id": "tYQlM64sRYaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles = read_random_json()\n",
        "text = random.choice(articles)[\"text\"]\n",
        "new_text = clean_text(text)"
      ],
      "metadata": {
        "id": "z2xfQINIRGby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(new_text)\n",
        "displacy.render(doc, style='span', jupyter=True, options={'distance': 88})\n"
      ],
      "metadata": {
        "id": "9Gi91VKwS3Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "'chess Grandmaster' or just 'Grandmaster' is not recognized as an entity. In Chess, titles are important\n",
        "Lost of cardinal values are unusable in current spans\n",
        "'International Master' is not recognized\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bmIlhIC2SpqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_text\n"
      ],
      "metadata": {
        "id": "msA9BxFnSspA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code takes more than an hour\n",
        "# data = (clean_text(json[\"text\"]) for json in get_json_readers())\n",
        "# result = nlp.pipe(data, n_process=-1, batch_size=1000)\n",
        "\n",
        "# srsly.write_gzip_jsonl('/somefile.gz', result)"
      ],
      "metadata": {
        "id": "n60bUGlzZG3L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}